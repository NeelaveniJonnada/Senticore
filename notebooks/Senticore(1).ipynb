{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mZEVxs6Uc-FR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLtdDr2xekKl",
        "outputId": "4fec9677-1f99-44c2-f697-9a41a5cbfd33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/datasets/MYDATASET.csv\")\n",
        "print(df.head())       # show first 5 rows\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOxXTMPpeby2",
        "outputId": "63b60387-5b35-4629-e3ab-363763308812"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  review_id product_id                  product_name  username  rating  \\\n",
            "0     R0001       P041  Bose Electronics Blender4112  user_058       4   \n",
            "1     R0002       P006        Samsung Sports Book588  user_024       3   \n",
            "2     R0003       P036       Adidas Coffee Cream3711  user_115       1   \n",
            "3     R0004       P022    Philips Wireless Phone5614  user_027       4   \n",
            "4     R0005       P003          Apple Home Cream6301  user_021       5   \n",
            "\n",
            "                           review_text sentiment          review_date  \\\n",
            "0  Excellent quality and fast delivery  Positive  2024-11-23 00:00:00   \n",
            "1               Decent product overall   Neutral  2023-08-27 00:00:00   \n",
            "2           Poor quality, disappointed  Negative  2025-02-16 00:00:00   \n",
            "3  Excellent quality and fast delivery  Positive  2024-01-03 00:00:00   \n",
            "4                      Would buy again  Positive  2025-06-25 00:00:00   \n",
            "\n",
            "   helpful_votes  verified_purchase  \n",
            "0             94              False  \n",
            "1             64               True  \n",
            "2             20               True  \n",
            "3             44               True  \n",
            "4             46               True  \n",
            "Index(['review_id', 'product_id', 'product_name', 'username', 'rating',\n",
            "       'review_text', 'sentiment', 'review_date', 'helpful_votes',\n",
            "       'verified_purchase'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UExTumo4gr3s",
        "outputId": "f2dd01fc-bfac-4d17-d60f-3e266fc5e076"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "Positive    5495\n",
            "Negative    2414\n",
            "Neutral     1591\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsBg0-_nS2Pp",
        "outputId": "9f70f055-690e-4348-85e8-2c6d7e2e61b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqNqZhxylw3I",
        "outputId": "fb91cb20-19d4-4210-bd9e-cb2b4cdcc981"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear svc\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 1: Data Augmentation (extra negatives)\n",
        "# ------------------------------------\n",
        "extra_negatives = [\n",
        "    \"This is the worst thing ever\",\n",
        "    \"Worst product I have bought\",\n",
        "    \"Absolutely horrible experience\",\n",
        "    \"Awful quality, very disappointed\",\n",
        "    \"Terrible and pathetic service\",\n",
        "    \"The worst purchase in my life\",\n",
        "    \"Extremely dissatisfied and angry\",\n",
        "    \"Completely useless and horrible\",\n",
        "    \"Utterly appalling, one of the worst experiences ever.\",\n",
        "    \"Disastrous beyond belief, completely intolerable.\",\n",
        "    \"A catastrophe, nothing redeemable about it.\",\n",
        "    \"Excruciatingly bad, I regret even trying it.\",\n",
        "    \"Absolutely horrendous, a complete failure.\",\n",
        "    \"Pathetic attempt, falls flat on every level.\",\n",
        "    \"A dreadful mess, impossible to recommend.\",\n",
        "    \"Painfully disappointing, worse than I imagined.\",\n",
        "    \"Worthless and frustrating, a waste of time.\",\n",
        "    \"An insult to quality, utterly unbearable.\",\n",
        "    \"bad thing.\"\n",
        "]\n",
        "extra_positives = [\n",
        "    \"Absolutely phenomenal experience, beyond expectations\",\n",
        "    \"A masterpiece, radiating brilliance in every aspect\",\n",
        "    \"Pure excellence, I couldn’t have asked for more\",\n",
        "    \"Incredibly uplifting, a once-in-a-lifetime experience\",\n",
        "    \"Heartwarming and empowering — truly remarkable\",\n",
        "    \"good experience\",\n",
        "    \"good thing\",\n",
        "    \"nice\",\n",
        "    \"I love this product, it is amazing!\",\n",
        "    \"love\",\n",
        "    \"Absolutely phenomenal experience, beyond expectations.\",\n",
        "    \"This left me speechless in the best way possible.\",\n",
        "    \"A masterpiece, radiating brilliance in every aspect.\",\n",
        "    \"A rare gem, truly delightful and inspiring.\",\n",
        "    \"Outstanding performance, unmatched and flawless.\",\n",
        "    \"Pure excellence, I couldn’t have asked for more.\",\n",
        "    \"This fills me with immense joy and admiration.\",\n",
        "    \"Incredibly uplifting, a once-in-a-lifetime experience.\",\n",
        "    \"Radiates positivity and brilliance all around.\",\n",
        "    \"Heartwarming and empowering — truly remarkable.\"\n",
        "]\n",
        "extra_neutrals = [\n",
        "    \"It was okay, nothing particularly special.\",\n",
        "    \"Neither good nor bad, just average.\",\n",
        "    \"It served its purpose, nothing more to add.\",\n",
        "    \"An ordinary outcome, as expected.\",\n",
        "    \"Not disappointing, but not impressive either.\",\n",
        "    \"The experience was passable, fairly standard.\",\n",
        "    \"It worked as intended, no surprises.\",\n",
        "    \"Just another day, quite routine and plain.\",\n",
        "    \"Neither thrilling nor dull, simply neutral.\",\n",
        "    \"Met the minimum expectations, no complaints.\",\n",
        "    \"It's okay , not bad\",\n",
        "    \"hi\", \"hii\", \"hiii\", \"hello\", \"hey\", \"yo\", \"sup\", \"good morning\", \"good evening\", \"good afternoon\"\n",
        "]\n",
        "df_extra = pd.DataFrame({\n",
        "    'review_text': extra_negatives + extra_positives + extra_neutrals,\n",
        "    'sentiment': (['Negative'] * len(extra_negatives)) +\n",
        "                 (['Positive'] * len(extra_positives)) +\n",
        "                 (['Neutral'] * len(extra_neutrals))\n",
        "\n",
        "})\n",
        "\n",
        "df = pd.concat([df, df_extra], ignore_index=True)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 2: Preprocessing (same as before)\n",
        "# ------------------------------------\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')   # optional but helps with lemmatization\n",
        "nltk.download('stopwords') # also needed since you’re using stopwords\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower().split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "df[\"clean_text\"] = df[\"review_text\"].apply(preprocess)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 3: Vectorizer with word + char n-grams\n",
        "# ------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
        "char_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), max_features=20000)\n",
        "\n",
        "vectorizer = FeatureUnion([(\"word\", word_vectorizer), (\"char\", char_vectorizer)])\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 4: Train-test split + Oversampling\n",
        "# ------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "X = df[\"clean_text\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train_vec, y_train)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 5: Train Linear SVM\n",
        "# ------------------------------------\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "model = LinearSVC(class_weight=\"balanced\", random_state=42)\n",
        "model.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 6: Prediction Function\n",
        "# ------------------------------------\n",
        "def predict_sentiment(text):\n",
        "    clean = preprocess(text)\n",
        "    vec = vectorizer.transform([clean])\n",
        "    return model.predict(vec)[0]\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 7: Custom Tests\n",
        "# ------------------------------------\n",
        "print(predict_sentiment(\"I love this product, it is amazing!\"))   # Positive\n",
        "print(predict_sentiment(\"This is the worst thing ever\"))          # Negative\n",
        "print(predict_sentiment(\"It was okay, not too bad\"))              # Neutral\n",
        "print(predict_sentiment(\"Terrible experience, very bad\"))         # Negative\n",
        "print(predict_sentiment(\"Absolutely fantastic, loved it\"))\n",
        "print(predict_sentiment(\"Hii\"))\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    clean = preprocess(text)\n",
        "    vec = vectorizer.transform([clean])\n",
        "    return model.predict(vec)[0]\n",
        "\n",
        "# Test again\n",
        "print(predict_sentiment(\"I love this product, it is amazing!\"))   # Positive\n",
        "print(predict_sentiment(\"This is the nasty thing ever\"))          # Negative\n",
        "print(predict_sentiment(\"It was okay, not too bad\"))\n",
        "print(predict_sentiment(\"hii\"))\n",
        "print(predict_sentiment(\"nice\"))\n",
        "import joblib\n",
        "\n",
        "# Save model & vectorizer\n",
        "joblib.dump(model, \"senticore_model.pkl\")\n",
        "joblib.dump(vectorizer, \"senticore_vectorizer.pkl\")\n",
        "\n",
        "print(\"✅ Model and vectorizer saved!\")\n",
        "# Load model & vectorizer\n",
        "model = joblib.load(\"senticore_model.pkl\")\n",
        "vectorizer = joblib.load(\"senticore_vectorizer.pkl\")\n",
        "\n",
        "print(\"✅ Model and vectorizer loaded!\")\n",
        "from google.colab import files\n",
        "files.download(\"senticore_model.pkl\")\n",
        "files.download(\"senticore_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "Vnj_R2UECHA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "4BZVUcGrCD5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------\n",
        "# Step 1: Data Augmentation (extra negatives)\n",
        "# ------------------------------------\n",
        "extra_negatives = [\n",
        "    \"This is the worst thing ever\",\n",
        "    \"Worst product I have bought\",\n",
        "    \"Absolutely horrible experience\",\n",
        "    \"Awful quality, very disappointed\",\n",
        "    \"Terrible and pathetic service\",\n",
        "    \"The worst purchase in my life\",\n",
        "    \"Extremely dissatisfied and angry\",\n",
        "    \"false\",\n",
        "    \"Completely useless and horrible\",\n",
        "    \"Utterly appalling, one of the worst experiences ever.\",\n",
        "    \"Disastrous beyond belief, completely intolerable.\",\n",
        "    \"A catastrophe, nothing redeemable about it.\",\n",
        "    \"Excruciatingly bad, I regret even trying it.\",\n",
        "    \"Absolutely horrendous, a complete failure.\",\n",
        "    \"Pathetic attempt, falls flat on every level.\",\n",
        "    \"A dreadful mess, impossible to recommend.\",\n",
        "    \"Painfully disappointing, worse than I imagined.\",\n",
        "    \"Worthless and frustrating, a waste of time.\",\n",
        "    \"An insult to quality, utterly unbearable.\",\n",
        "    \"bad thing.\",\n",
        "    \"nasty.\",\n",
        "    \"poor.\"\n",
        "]\n",
        "extra_positives = [\n",
        "    \"Absolutely phenomenal experience, beyond expectations\",\n",
        "    \"A masterpiece, radiating brilliance in every aspect\",\n",
        "    \"Pure excellence, I couldn’t have asked for more\",\n",
        "    \"Incredibly uplifting, a once-in-a-lifetime experience\",\n",
        "    \"Heartwarming and empowering — truly remarkable\",\n",
        "    \"good experience\",\n",
        "    \"best thing\",\n",
        "    \"true\",\n",
        "    \"I love this product, it is amazing!\",\n",
        "    \"love\",\n",
        "    \"great\",\n",
        "    \"nice\",\n",
        "    \"amazing\",\n",
        "    \"negative\",\n",
        "    \"Absolutely phenomenal experience, beyond expectations.\",\n",
        "    \"This left me speechless in the best way possible.\",\n",
        "    \"A masterpiece, radiating brilliance in every aspect.\",\n",
        "    \"A rare gem, truly delightful and inspiring.\",\n",
        "    \"Outstanding performance, unmatched and flawless.\",\n",
        "    \"Pure excellence, I couldn’t have asked for more.\",\n",
        "    \"This fills me with immense joy and admiration.\",\n",
        "    \"Incredibly uplifting, a once-in-a-lifetime experience.\",\n",
        "    \"Radiates positivity and brilliance all around.\",\n",
        "    \"Heartwarming and empowering — truly remarkable.\"\n",
        "]\n",
        "extra_neutrals = [\n",
        "    \"It was okay, nothing particularly special.\",\n",
        "    \"Neither good nor bad, just average.\",\n",
        "    \"It served its purpose, nothing more to add.\",\n",
        "    \"An ordinary outcome, as expected.\",\n",
        "    \"Not disappointing, but not impressive either.\",\n",
        "    \"The experience was passable, fairly standard.\",\n",
        "    \"It worked as intended, no surprises.\",\n",
        "    \"Just another day, quite routine and plain.\",\n",
        "    \"Neither thrilling nor dull, simply neutral.\",\n",
        "    \"Met the minimum expectations, no complaints.\",\n",
        "    \"It's okay , not bad\",\n",
        "    \"hi\", \"hii\", \"hiii\", \"hello\", \"hey\", \"yo\", \"sup\", \"good morning\", \"good evening\", \"good afternoon\"\n",
        "]\n",
        "df_extra = pd.DataFrame({\n",
        "    'review_text': extra_negatives + extra_positives + extra_neutrals,\n",
        "    'sentiment': (['Negative'] * len(extra_negatives)) +\n",
        "                 (['Positive'] * len(extra_positives)) +\n",
        "                 (['Neutral'] * len(extra_neutrals))\n",
        "\n",
        "})\n",
        "\n",
        "df = pd.concat([df, df_extra], ignore_index=True)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 2: Preprocessing (same as before)\n",
        "# ------------------------------------\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')   # optional but helps with lemmatization\n",
        "nltk.download('stopwords') # also needed since you’re using stopwords\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower().split()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "df[\"clean_text\"] = df[\"review_text\"].apply(preprocess)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 3: Vectorizer with word + char n-grams\n",
        "# ------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
        "char_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), max_features=20000)\n",
        "\n",
        "vectorizer = FeatureUnion([(\"word\", word_vectorizer), (\"char\", char_vectorizer)])\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 4: Train-test split + Oversampling\n",
        "# ------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "X = df[\"clean_text\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train_vec, y_train)\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 5: Train Linear SVM\n",
        "# ------------------------------------\n",
        "# Step 5: Train Decision Tree\n",
        "# ------------------------------------\n",
        "# ------------------------------------\n",
        "# Step 5: Train Random Forest\n",
        "# ------------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=300,        # number of trees\n",
        "    max_depth=50,            # prevent overfitting\n",
        "    class_weight=\"balanced\", # handle class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1                # use all CPU cores\n",
        ")\n",
        "\n",
        "model.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 6: Prediction Function\n",
        "# ------------------------------------\n",
        "def predict_sentiment(text):\n",
        "    clean = preprocess(text)\n",
        "    vec = vectorizer.transform([clean])\n",
        "\n",
        "    pred = model.predict(vec)[0]\n",
        "    probs = model.predict_proba(vec)[0]\n",
        "\n",
        "    # map probabilities to class labels\n",
        "    prob_dict = {\n",
        "        cls.lower(): round(float(p * 100), 2)\n",
        "        for cls, p in zip(model.classes_, probs)\n",
        "    }\n",
        "\n",
        "    return pred, prob_dict\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "# Step 7: Custom Tests\n",
        "# ------------------------------------\n",
        "print(predict_sentiment(\"I love this product, it is amazing!\"))   # Positive\n",
        "print(predict_sentiment(\"This is the worst thing ever\"))          # Negative\n",
        "print(predict_sentiment(\"It was okay, not too bad\"))              # Neutral\n",
        "print(predict_sentiment(\"Terrible experience, very bad\"))         # Negative\n",
        "print(predict_sentiment(\"Absolutely fantastic, loved it\"))\n",
        "print(predict_sentiment(\"Hii\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2621868-18cf-46e7-ec43-1a3b6da6e246",
        "id": "LknQv5tA-DL6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.92      0.96       547\n",
            "     Neutral       0.90      1.00      0.94       386\n",
            "    Positive       1.00      1.00      1.00      1164\n",
            "\n",
            "    accuracy                           0.98      2097\n",
            "   macro avg       0.96      0.97      0.97      2097\n",
            "weighted avg       0.98      0.98      0.98      2097\n",
            "\n",
            "[[ 502   45    0]\n",
            " [   1  385    0]\n",
            " [   0    0 1164]]\n",
            "('Positive', {'negative': 0.0, 'neutral': 0.0, 'positive': 100.0})\n",
            "('Negative', {'negative': 99.68, 'neutral': 0.27, 'positive': 0.05})\n",
            "('Neutral', {'negative': 0.35, 'neutral': 99.07, 'positive': 0.57})\n",
            "('Negative', {'negative': 90.17, 'neutral': 3.46, 'positive': 6.36})\n",
            "('Positive', {'negative': 32.28, 'neutral': 14.05, 'positive': 53.67})\n",
            "('Neutral', {'negative': 2.31, 'neutral': 95.63, 'positive': 2.07})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_sentiment(\"I love this product, it is amazing!\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoEXHVTNCgwO",
        "outputId": "c3f760d5-1e34-4804-eca3-a3c547b16ada"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Positive', {'negative': 0.0, 'neutral': 0.0, 'positive': 100.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment_with_proba(text):\n",
        "    clean = preprocess(text)\n",
        "    vec = vectorizer.transform([clean])\n",
        "\n",
        "    # prediction\n",
        "    pred = model.predict(vec)[0]\n",
        "\n",
        "    # probabilities\n",
        "    probs = model.predict_proba(vec)[0]\n",
        "    prob_dict = {\n",
        "        cls.lower(): round(float(p * 100), 2)\n",
        "        for cls, p in zip(model.classes_, probs)\n",
        "    }\n",
        "\n",
        "    return pred, prob_dict\n",
        "print(predict_sentiment(\"I love this product, it is amazing!\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKLhl6Mu-SXF",
        "outputId": "d377aead-4506-41fa-9649-03fc9ac1a67a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Positive', {'negative': 0.0, 'neutral': 0.0, 'positive': 100.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_sentiment(\"nasty\"))\n",
        "print(predict_sentiment(\"great\"))\n",
        "print(predict_sentiment(\"amazing\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZU9eV9BRR3R",
        "outputId": "74dacb1a-a5d4-44d9-e136-40969d688822"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Negative', {'negative': 86.21, 'neutral': 12.98, 'positive': 0.81})\n",
            "('Positive', {'negative': 1.14, 'neutral': 9.97, 'positive': 88.88})\n",
            "('Positive', {'negative': 0.7, 'neutral': 6.74, 'positive': 92.56})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save model & vectorizer\n",
        "joblib.dump(model, \"senticore_model.pkl\")\n",
        "joblib.dump(vectorizer, \"senticore_vectorizer.pkl\")\n",
        "\n",
        "print(\"✅ Model and vectorizer saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQOZP1IcHU3F",
        "outputId": "c9245312-09a2-479a-c35e-a03fdfaa64b0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and vectorizer saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(\"senticore_model.pkl\")\n",
        "vectorizer = joblib.load(\"senticore_vectorizer.pkl\")\n",
        "\n",
        "print(\"✅ Model and vectorizer loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw5lIr6XHs_G",
        "outputId": "b4a08529-6ddc-4801-c9b3-345b9da6341f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and vectorizer loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"senticore_model.pkl\")\n",
        "files.download(\"senticore_vectorizer.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-eC_1rLxUOP1",
        "outputId": "9804fb1e-bee7-4824-821d-513bd531ff90"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_604f73fe-873b-4df9-bbee-e8faee94e284\", \"senticore_model.pkl\", 3097217)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e43a2913-df8c-45a0-a174-ecc3099b7cad\", \"senticore_vectorizer.pkl\", 117172)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}